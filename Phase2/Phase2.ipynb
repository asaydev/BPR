{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 0 : Preprocessing the data</h1>\n",
    "<div>In this section we prepare the text and also we vectorize the documents both train and test.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "english_words_count = {}\n",
    "data = pd.read_csv('train.csv')\n",
    "descs = data.iloc[:, 1].values\n",
    "titles = data.iloc[:, 14].values\n",
    "for i in range(len(titles)):\n",
    "    A = titles[i].split(' ')\n",
    "    B = descs[i].split(' ')\n",
    "    for term in A:\n",
    "        if(term in english_words_count):\n",
    "            english_words_count[term] += 1\n",
    "        else:\n",
    "            english_words_count[term] = 1\n",
    "    for term in B:\n",
    "        if(term in english_words_count):\n",
    "            english_words_count[term] += 1\n",
    "        else:\n",
    "            english_words_count[term] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stopwords_count = []\n",
    "for term in english_words_count:\n",
    "    english_stopwords_count.append((term, english_words_count[term]))\n",
    "english_stopwords_count.sort(key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'of', 'to', 'a', 'in', '--']\n"
     ]
    }
   ],
   "source": [
    "english_stopwords = []\n",
    "for i in range(7):\n",
    "    english_stopwords.append(english_stopwords_count[i][0])\n",
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from PersianStemmer import PersianStemmer\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def prepare_text(raw_text, lang = 'fa', stem = True):\n",
    "    if(lang == 'en'):\n",
    "        tokens = word_tokenize(raw_text)\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        porter = PorterStemmer()\n",
    "        prepared_text = []\n",
    "        for word in tokens:\n",
    "            if(word not in english_stopwords):\n",
    "                if(stem):\n",
    "                    prepared_text.append(porter.stem(word))\n",
    "                else:\n",
    "                    prepared_text.append(word)\n",
    "        return prepared_text\n",
    "    \n",
    "    elif(lang == 'fa'):\n",
    "        normalizer = Normalizer()\n",
    "        normalized_text = normalizer.normalize(raw_text)\n",
    "        tokenizer = WordTokenizer()\n",
    "        tokenized_text = tokenizer.tokenize(normalized_text)\n",
    "        ps = PersianStemmer()\n",
    "        prepared_text = []\n",
    "        for word in tokenized_text:\n",
    "            if(word[0] >= \"آ\" and word[0] <= \"ی\" and word not in persian_stopwords):\n",
    "                if(stem):\n",
    "                    prepared_text.append(ps.run(word))\n",
    "                else:\n",
    "                    prepared_text.append(word)\n",
    "        return prepared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positional_index = {}\n",
    "positional_index_test = {}\n",
    "positional_index_ted = {}\n",
    "\n",
    "def add_positional(term, docid, position, t, data_set = 'train'):\n",
    "    if(data_set == 'train'):\n",
    "        if(term not in positional_index.keys()):\n",
    "            positional_index[term] = {}\n",
    "            positional_index[term][int(docid)] = {}\n",
    "            positional_index[term][int(docid)][t] = [position]\n",
    "        else:\n",
    "            if(int(docid) not in positional_index[term].keys()):\n",
    "                positional_index[term][int(docid)] = {}\n",
    "                positional_index[term][int(docid)][t] = [position]\n",
    "            else:\n",
    "                if(t not in positional_index[term][int(docid)]):\n",
    "                    positional_index[term][int(docid)][t] = [position]\n",
    "                else:\n",
    "                    positional_index[term][int(docid)][t].append(position)\n",
    "    elif(data_set == 'test'):\n",
    "        if(term not in positional_index_test.keys()):\n",
    "            positional_index_test[term] = {}\n",
    "            positional_index_test[term][int(docid)] = {}\n",
    "            positional_index_test[term][int(docid)][t] = [position]\n",
    "        else:\n",
    "            if(int(docid) not in positional_index_test[term].keys()):\n",
    "                positional_index_test[term][int(docid)] = {}\n",
    "                positional_index_test[term][int(docid)][t] = [position]\n",
    "            else:\n",
    "                if(t not in positional_index_test[term][int(docid)]):\n",
    "                    positional_index_test[term][int(docid)][t] = [position]\n",
    "                else:\n",
    "                    positional_index_test[term][int(docid)][t].append(position)\n",
    "    elif(data_set == 'ted'):\n",
    "        if(term not in positional_index_ted.keys()):\n",
    "            positional_index_ted[term] = {}\n",
    "            positional_index_ted[term][int(docid)] = {}\n",
    "            positional_index_ted[term][int(docid)][t] = [position]\n",
    "        else:\n",
    "            if(int(docid) not in positional_index_ted[term].keys()):\n",
    "                positional_index_ted[term][int(docid)] = {}\n",
    "                positional_index_ted[term][int(docid)][t] = [position]\n",
    "            else:\n",
    "                if(t not in positional_index_ted[term][int(docid)]):\n",
    "                    positional_index_ted[term][int(docid)][t] = [position]\n",
    "                else:\n",
    "                    positional_index_ted[term][int(docid)][t].append(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docids = []\n",
    "def construct_positional_indexes(docs_path, data_format = 'xml', data_set = 'train'):\n",
    "    if(data_format == 'xml'):\n",
    "        mydoc = minidom.parse(docs_path)\n",
    "        texts = mydoc.getElementsByTagName('text')\n",
    "        titles = mydoc.getElementsByTagName('title')\n",
    "        ids = mydoc.getElementsByTagName('id')\n",
    "        tree = ET.parse(docs_path)\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            for c in child:\n",
    "                if(c.tag == '{http://www.mediawiki.org/xml/export-0.10/}id'):\n",
    "                    docids.append(int(c.text))\n",
    "        for i in range(len(texts)):\n",
    "            A = prepare_text(titles[i].firstChild.data)\n",
    "            B = prepare_text(texts[i].firstChild.data)\n",
    "            for j in range(len(A)):\n",
    "                add_positional(A[j], docids[i], j, 'title', data_set)\n",
    "            for j in range(len(B)):\n",
    "                add_positional(B[j], docids[i], j, 'text', data_set)\n",
    "    \n",
    "    elif(data_format == 'csv'):\n",
    "        data = pd.read_csv(docs_path)\n",
    "        descs = data.iloc[:, 1].values\n",
    "        titles = data.iloc[:, 14].values\n",
    "        for i in range(len(descs)):\n",
    "            docids.append(i)\n",
    "            A = prepare_text(titles[i], 'en')\n",
    "            B = prepare_text(descs[i], 'en')\n",
    "            for j in range(len(A)):\n",
    "                add_positional(A[j], i, j, 'title', data_set)\n",
    "            for j in range(len(B)):\n",
    "                add_positional(B[j], i, j, 'text', data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "construct_positional_indexes('train.csv', 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docids = []\n",
    "construct_positional_indexes('test.csv', 'csv', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docids = []\n",
    "construct_positional_indexes('ted_talks.csv', 'csv', 'ted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "train_vectors = []\n",
    "train_classes = []\n",
    "def vectorize(docs_path):\n",
    "        data = pd.read_csv(docs_path)\n",
    "        descs = data.iloc[:, 1].values\n",
    "        titles = data.iloc[:, 14].values\n",
    "        views = data.iloc[:, 16].values\n",
    "        N = len(descs)\n",
    "        idf = {}\n",
    "        for term in positional_index:\n",
    "            idf[term] = log(N / len(positional_index[term].keys()), 10)\n",
    "        for i in range(N):\n",
    "            vector = []\n",
    "            for term in positional_index.keys():\n",
    "                tf = 0\n",
    "                if(i in positional_index[term].keys()):\n",
    "                    if('title' in positional_index[term][i].keys()):\n",
    "                        tf += len(positional_index[term][i]['title'])\n",
    "                    if('text' in positional_index[term][i].keys()):\n",
    "                        tf += len(positional_index[term][i]['text'])\n",
    "                vector.append(tf * idf[term])\n",
    "            train_vectors.append(vector)\n",
    "            train_classes.append(views[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorize('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_vectors = []\n",
    "test_classes = []\n",
    "\n",
    "def vectorize_test(docs_path):\n",
    "        data = pd.read_csv(docs_path)\n",
    "        descs = data.iloc[:, 1].values\n",
    "        titles = data.iloc[:, 14].values\n",
    "        views = data.iloc[:, 16].values\n",
    "        N = len(descs)\n",
    "        M = 2295\n",
    "        idf = {}\n",
    "        for term in positional_index:\n",
    "            idf[term] = log(M / len(positional_index[term].keys()), 10)\n",
    "        for i in range(N):\n",
    "            vector = []\n",
    "            for term in positional_index.keys():\n",
    "                tf = 0\n",
    "                if(term in positional_index_test.keys()):\n",
    "                    if(i in positional_index_test[term].keys()):\n",
    "                        if('title' in positional_index_test[term][i].keys()):\n",
    "                            tf += len(positional_index_test[term][i]['title'])\n",
    "                        if('text' in positional_index_test[term][i].keys()):\n",
    "                            tf += len(positional_index_test[term][i]['text'])\n",
    "                vector.append(tf * idf[term])\n",
    "\n",
    "            test_vectors.append(vector)\n",
    "            test_classes.append(views[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorize_test('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ted_vectors = []\n",
    "ted_classes = []\n",
    "\n",
    "def vectorize_ted(docs_path):\n",
    "        data = pd.read_csv(docs_path)\n",
    "        descs = data.iloc[:, 1].values\n",
    "        titles = data.iloc[:, 14].values\n",
    "        views = data.iloc[:, 16].values\n",
    "        N = len(descs)\n",
    "        M = 2295\n",
    "        idf = {}\n",
    "        for term in positional_index:\n",
    "            idf[term] = log(M / len(positional_index[term].keys()), 10)\n",
    "        for i in range(N):\n",
    "            vector = []\n",
    "            for term in positional_index.keys():\n",
    "                tf = 0\n",
    "                if(term in positional_index_ted.keys()):\n",
    "                    if(i in positional_index_ted[term].keys()):\n",
    "                        if('title' in positional_index_ted[term][i].keys()):\n",
    "                            tf += len(positional_index_ted[term][i]['title'])\n",
    "                        if('text' in positional_index_ted[term][i].keys()):\n",
    "                            tf += len(positional_index_ted[term][i]['text'])\n",
    "                vector.append(tf * idf[term])\n",
    "\n",
    "            ted_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorize_ted('ted_talks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 1 : Classifiers</h1>\n",
    "<div>In this section we write 4 different classifiers. Naive Bayes, KNN, SVM and RandomForest. We test these classifiers on training data and validation data.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "train_vectors1, valid_vectors1, train_classes1, valid_classes1 = sklearn.model_selection.train_test_split(train_vectors, train_classes, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "views = data.iloc[:, 16].values\n",
    "N = len(views)\n",
    "p_1 = 0\n",
    "p_2 = 0\n",
    "for view in views:\n",
    "    if(int(view) == 1):\n",
    "        p_1 += 1\n",
    "    elif(int(view) == -1):\n",
    "        p_2 += 1\n",
    "p_1 /= N\n",
    "p_2 /= N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A_1 = {}\n",
    "A_2 = {}\n",
    "B_1 = 0\n",
    "B_2 = 0\n",
    "\n",
    "for term in positional_index:\n",
    "    for i in positional_index[term]:\n",
    "        if(int(train_classes[i]) == 1):\n",
    "            if('title' in positional_index[term][i]):\n",
    "                B_1 += len(positional_index[term][i]['title'])\n",
    "            if('text' in positional_index[term][i]):\n",
    "                B_1 += len(positional_index[term][i]['text'])\n",
    "        elif(int(train_classes[i]) == -1):\n",
    "            if('title' in positional_index[term][i]):\n",
    "                B_2 += len(positional_index[term][i]['title'])\n",
    "            if('text' in positional_index[term][i]):\n",
    "                B_2 += len(positional_index[term][i]['text'])\n",
    "                \n",
    "for term in positional_index:\n",
    "    A_1[term] = 0\n",
    "    A_2[term] = 0\n",
    "    for i in positional_index[term]:\n",
    "        if(int(train_classes[i]) == 1):\n",
    "            if('title' in positional_index[term][i]):\n",
    "                A_1[term] += len(positional_index[term][i]['title'])\n",
    "            if('text' in positional_index[term][i]):\n",
    "                A_1[term] += len(positional_index[term][i]['text'])\n",
    "        elif(int(train_classes[i]) == -1):\n",
    "            if('title' in positional_index[term][i]):\n",
    "                A_2[term] += len(positional_index[term][i]['title'])\n",
    "            if('text' in positional_index[term][i]):\n",
    "                A_2[term] += len(positional_index[term][i]['text'])\n",
    "    A_1[term] = (A_1[term] + 1) / (B_1 + len(positional_index.keys()))\n",
    "    A_2[term] = (A_2[term] + 1) / (B_2 + len(positional_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def NB(data_set = 'train'):\n",
    "    if(data_set == 'train'):\n",
    "        P = positional_index\n",
    "        T = train_vectors\n",
    "    elif(data_set == 'test'):\n",
    "        P = positional_index_test\n",
    "        T = test_vectors\n",
    "    res = []\n",
    "    for i in range(len(T)):\n",
    "        res.append([log(p_1, 10), log(p_2, 10)])\n",
    "    for term in P:\n",
    "        for i in P[term]:\n",
    "            if(term in A_1):\n",
    "                res[i][0] += log(A_1[term], 10)\n",
    "            if(term in A_2):\n",
    "                res[i][1] += log(A_2[term], 10)\n",
    "                \n",
    "    predict = []\n",
    "    for i in range(len(T)):\n",
    "        if(res[i][0] > res[i][1]):\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(-1)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(a, b):\n",
    "    res = 0\n",
    "    for i in range(len(a)):\n",
    "        if(int(a[i]) == int(b[i])):\n",
    "            res += 1\n",
    "    return res / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB_predict = NB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "print(NB_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9507625272331155\n"
     ]
    }
   ],
   "source": [
    "print(correct(NB_predict, train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eucilidian_distance(a, b):\n",
    "    distance = 0\n",
    "    for i in range(len(a)):\n",
    "        distance += (a[i] - b[i]) ** 2\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn(vector, k):\n",
    "    t = []\n",
    "    for i in range(len(train_classes1)):\n",
    "        d = eucilidian_distance(vector, train_vectors[i])\n",
    "        t.append((d, train_classes1[i]))\n",
    "    t.sort(key=lambda x:x[0])\n",
    "    selected = t[0:k]\n",
    "    a = 0\n",
    "    b = 0\n",
    "    for item in selected:\n",
    "        if(int(item[1]) == 1):\n",
    "            a += 1\n",
    "        elif(int(item[1]) == -1):\n",
    "            b += 1\n",
    "    if(a > b):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_all(k, data_set = 'valid'):\n",
    "    if(data_set == 'valid'):\n",
    "        T = valid_vectors1\n",
    "    elif(data_set == 'test'):\n",
    "        T = test_vectors\n",
    "    elif(data_set == 'train'):\n",
    "        T = train_vectors\n",
    "    predict = []\n",
    "    for i in range(len(T)):\n",
    "        predict.append(knn(T[i], k))\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_predict1 = knn_all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5304347826086957\n"
     ]
    }
   ],
   "source": [
    "print(correct(knn_predict1, valid_classes1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_predict5 = knn_all(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(correct(knn_predict5, valid_classes1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_predict9 = knn_all(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5434782608695652\n"
     ]
    }
   ],
   "source": [
    "print(correct(knn_predict9, valid_classes1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "regr1 = svm.SVC(kernel='rbf', C=0.5)\n",
    "regr1.fit(train_vectors1, train_classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_predict1 = regr1.predict(valid_vectors1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(correct(svm_predict1, valid_classes1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "regr2 = svm.SVC(kernel='rbf', C=1)\n",
    "regr2.fit(train_vectors1, train_classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_predict2 = regr2.predict(valid_vectors1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "print(correct(svm_predict2, valid_classes1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.5, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "regr3 = svm.SVC(kernel='rbf', C=1.5)\n",
    "regr3.fit(train_vectors1, train_classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_predict3 = regr3.predict(valid_vectors1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7217391304347827\n"
     ]
    }
   ],
   "source": [
    "print(correct(svm_predict3, valid_classes1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=2, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "regr4 = svm.SVC(kernel='rbf', C=2)\n",
    "regr4.fit(train_vectors1, train_classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_predict4 = regr4.predict(valid_vectors1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7086956521739131\n"
     ]
    }
   ],
   "source": [
    "print(correct(svm_predict4, valid_classes1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=10, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=10)\n",
    "clf.fit(train_vectors, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_forest_predict = clf.predict(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8453159041394336\n"
     ]
    }
   ],
   "source": [
    "print(correct(random_forest_predict, train_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 2 : Modifying phase 1</h1>\n",
    "<div>In this section, we classify the documents from ted_talks.csv at first. Then we add the views argument to the search and proximate_search functions from phase 1.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ted_views = clf.predict(ted_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1  1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(ted_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def search(query, views, lang = 'en',weight=2, returned = 10, all_docs = None):\n",
    "    relevant_docs = []\n",
    "    N = len(docids)\n",
    "    t = len(positional_index_ted.keys())\n",
    "\n",
    "    idf = []\n",
    "    words = prepare_text(query, lang)\n",
    "            \n",
    "    q = []\n",
    "    all1 = []\n",
    "    for word in positional_index_ted.keys():\n",
    "        all1.append(word)\n",
    "        \n",
    "\n",
    "    query_terms = []\n",
    "    for word in words:\n",
    "        if(word not in query_terms):\n",
    "            query_terms.append(word)\n",
    "\n",
    "    q1 = []\n",
    "    for term in query_terms:\n",
    "        q1.append(1 + math.log(words.count(term), 10))\n",
    "    \n",
    "\n",
    "    for term in query_terms:\n",
    "\n",
    "        temp1 = 0\n",
    "        for d in docids:\n",
    "            if(d in positional_index_ted[term].keys()):\n",
    "                temp1 += 1\n",
    "\n",
    "        idf.append(math.log(N / (temp1), 10))\n",
    "        \n",
    "    score = []\n",
    "    docids1 = docids\n",
    "    if(all_docs != None):\n",
    "        docids1 = all_docs\n",
    "    \n",
    "    for i in docids1:\n",
    "        if(ted_views[i] != views):\n",
    "            continue\n",
    "        if(lang == 'en' and i > 3000):\n",
    "            continue\n",
    "        if(lang == 'fa' and i < 3000):\n",
    "            continue\n",
    "        flag = False\n",
    "        f = True\n",
    "\n",
    "        if(flag == True):\n",
    "            continue\n",
    "        \n",
    "        if(f == False):\n",
    "            continue\n",
    "                \n",
    "        s = 0\n",
    "        length_title = 0\n",
    "        length_text = 0\n",
    "        for term in query_terms:\n",
    "            if(i not in positional_index_ted[term]):\n",
    "                length_title += 0\n",
    "                length_text += 0\n",
    "            else:\n",
    "                if('title' not in positional_index_ted[term][i].keys()):\n",
    "                    length_title += 0\n",
    "                else:\n",
    "                    length_title += (len(positional_index_ted[term][i]['title'])) ** 2\n",
    "\n",
    "                if('text' not in positional_index_ted[term][i].keys()):\n",
    "                    length_text += 0\n",
    "                else:\n",
    "                    length_text += (len(positional_index_ted[term][i]['text'])) ** 2\n",
    "        length_title = math.sqrt(length_title)\n",
    "        length_text = math.sqrt(length_text)\n",
    "        v = 0\n",
    "        for term in query_terms:\n",
    "            if(i not in positional_index_ted[term]):\n",
    "                s += 0\n",
    "            else:\n",
    "                if('title' not in positional_index_ted[term][i].keys()):\n",
    "                    s += 0\n",
    "                else:\n",
    "                    temp = len(positional_index_ted[term][i]['title'])\n",
    "                    s += weight * ((1 + math.log(temp, 10)) * idf[v] * q1[v]) / length_title\n",
    "\n",
    "                if('text' not in positional_index_ted[term][i].keys()):\n",
    "                    s += 0\n",
    "                else:\n",
    "                    temp = len(positional_index_ted[term][i]['text'])\n",
    "                    s += ((1 + math.log(temp, 10)) * idf[v] * q1[v]) / length_text\n",
    "            v += 1\n",
    "        score.append([i, s])\n",
    "\n",
    "    result = sorted(score, key = lambda x:x[1], reverse = True)\n",
    "    z = min(returned, len(result))\n",
    "    for i in range(z):\n",
    "        relevant_docs.append(result[i][0])\n",
    "        \n",
    "    return relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 642, 1244, 3, 5, 7, 10, 11, 12, 13]\n",
      "[277, 278, 2455, 331, 1398, 1618, 1716, 895, 324, 335]\n"
     ]
    }
   ],
   "source": [
    "print(search('Sir Ken Robinson', -1))\n",
    "print(search('walking a dog', -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proximate_search(query, views, window, lang = 'en', weight=2, returned = 10):\n",
    "    words = prepare_text(query, lang)\n",
    "    docs = []\n",
    "    relevant_docs = []\n",
    "    for word in words:\n",
    "        docs.append(set(positional_index_ted[word].keys()))\n",
    "    all_docs = set.intersection(*docs)\n",
    "    for doc in all_docs:\n",
    "        posting_lists = []\n",
    "        flag = False\n",
    "        for word in words:\n",
    "            if('text' in positional_index_ted[word][doc]):\n",
    "                posting_lists.append(positional_index_ted[word][doc]['text'])\n",
    "            else:\n",
    "                flag = True\n",
    "                break\n",
    "        if(flag):\n",
    "            continue\n",
    "        for p in posting_lists[0]:\n",
    "            flag2 = False\n",
    "            for i in range(1, len(posting_lists)):\n",
    "                if(len(list(filter(lambda x : x <= (p + window) and x >= (p - window), posting_lists[i]))) == 0):\n",
    "                    flag2 = True\n",
    "                    break\n",
    "            if(flag2):\n",
    "                continue\n",
    "            else:\n",
    "                relevant_docs.append(doc)\n",
    "                break\n",
    "    return search(query, views, lang, weight, returned, all_docs = relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43]\n"
     ]
    }
   ],
   "source": [
    "print(proximate_search(\"demonic lyrics\", 1, 2, 'en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 3 : Scoring system</h1>\n",
    "<div>In this section we implement Accuracy, F measure, Recall and precision of the classifiers on the test data</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predicted, original):\n",
    "    N = len(predicted)\n",
    "    res = 0\n",
    "    for i in range(N):\n",
    "        if(predicted[i] == original[i]):\n",
    "            res += 1\n",
    "    return res / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(predicted, original):\n",
    "    a = 0\n",
    "    b = 0\n",
    "    N = len(predicted)\n",
    "    for i in range(N):\n",
    "        if(predicted[i] == 1):\n",
    "            b += 1\n",
    "            if(original[i] == 1):\n",
    "                a += 1\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall(predicted, original):\n",
    "    a = 0\n",
    "    b = 0\n",
    "    N = len(predicted)\n",
    "    for i in range(N):\n",
    "        if(original[i] == 1):\n",
    "            b += 1\n",
    "            if(predicted[i] == 1):\n",
    "                a += 1\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F_measure(predicted, original):\n",
    "    pr = precision(predicted, original)\n",
    "    re = recall(predicted, original)\n",
    "    return (2 * pr * re) / (pr + re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_train = NB()\n",
    "NB_test = NB('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9507625272331155\n",
      "0.9748700173310225\n",
      "0.9748700173310225\n",
      "0.9748700173310225\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(NB_train, train_classes))\n",
    "print(precision(NB_train, train_classes))\n",
    "print(recall(NB_train, train_classes))\n",
    "print(F_measure(NB_train, train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6235294117647059\n",
      "0.7272727272727273\n",
      "0.7272727272727273\n",
      "0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(NB_test, test_classes))\n",
    "print(precision(NB_test, test_classes))\n",
    "print(recall(NB_test, test_classes))\n",
    "print(F_measure(NB_test, test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_train = knn_all(9, 'train')\n",
    "knn_test = knn_all(9, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(accuracy(knn_train, train_classes))\n",
    "print(precision(knn_train, train_classes))\n",
    "print(recall(knn_train, train_classes))\n",
    "print(F_measure(knn_train, train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(accuracy(knn_test, test_classes))\n",
    "print(precision(knn_test, test_classes))\n",
    "print(recall(knn_test, test_classes))\n",
    "print(F_measure(knn_test, test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_train = regr3.predict(train_vectors)\n",
    "svm_test = regr3.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9703703703703703\n",
      "0.9740034662045061\n",
      "0.9740034662045061\n",
      "0.9740034662045061\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(svm_train, train_classes))\n",
    "print(precision(svm_train, train_classes))\n",
    "print(recall(svm_train, train_classes))\n",
    "print(F_measure(svm_train, train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6588235294117647\n",
      "0.6115702479338843\n",
      "0.6115702479338843\n",
      "0.6115702479338843\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(svm_test, test_classes))\n",
    "print(precision(svm_test, test_classes))\n",
    "print(recall(svm_test, test_classes))\n",
    "print(F_measure(svm_test, test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_train = clf.predict(train_vectors)\n",
    "RF_test = clf.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8453159041394336\n",
      "0.8024263431542461\n",
      "0.8024263431542461\n",
      "0.8024263431542461\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(RF_train, train_classes))\n",
    "print(precision(RF_train, train_classes))\n",
    "print(recall(RF_train, train_classes))\n",
    "print(F_measure(RF_train, train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6196078431372549\n",
      "0.5702479338842975\n",
      "0.5702479338842975\n",
      "0.5702479338842975\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(RF_test, test_classes))\n",
    "print(precision(RF_test, test_classes))\n",
    "print(recall(RF_test, test_classes))\n",
    "print(F_measure(RF_test, test_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
