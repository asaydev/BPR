{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 1 : Data Pre-processing</h1>\n",
    "<div>In this section we build a function that accepts raw text extracted from a doc or a query and after applying normalization, tokenization, stopword removal and stemming returns an array of tokens (token stream).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b4dca0a62ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprepared_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from PersianStemmer import PersianStemmer\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def prepare_text(raw_text, lang = 'fa'):\n",
    "    if(lang == 'en'):\n",
    "        tokens = word_tokenize(raw_text)\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        porter = PorterStemmer()\n",
    "        prepared_text = []\n",
    "        for word in tokens:\n",
    "            if(tokens.count(word) < 1/15 or len(tokens) < 120):\n",
    "                prepared_text.append(porter.stem(word))\n",
    "        return prepared_text\n",
    "    \n",
    "    elif(lang == 'fa'):\n",
    "        normalizer = Normalizer()\n",
    "        normalized_text = normalizer.normalize(raw_text)\n",
    "        tokenizer = WordTokenizer()\n",
    "        tokenized_text = tokenizer.tokenize(normalized_text)\n",
    "        ps = PersianStemmer()\n",
    "        prepared_text = []\n",
    "        for word in tokenized_text:\n",
    "            if(word[0] >= \"آ\" and word[0] <= \"ی\" and (tokenized_text.count(word) < 1/15 or len(tokenized_text) < 120)):\n",
    "                prepared_text.append(ps.run(word))\n",
    "        return prepared_text\n",
    "\n",
    "raw_text = input()\n",
    "print(prepare_text(raw_text, 'en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 2 : Indexing</h1>\n",
    "<div>In this section we build a function for positional indexing and biword indexing. We then save this indexes and also we have functions for adding and deleting docs in a dynamic way meaning that you don't need to repeat the indexing process from the beginning.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positional_index = {}\n",
    "def add_positional(term, docid, position, t):\n",
    "    if(term not in positional_index.keys()):\n",
    "        positional_index[term] = {}\n",
    "        positional_index[term][int(docid)] = {}\n",
    "        positional_index[term][int(docid)][t] = [position]\n",
    "    else:\n",
    "        if(int(docid) not in positional_index[term].keys()):\n",
    "            positional_index[term][int(docid)] = {}\n",
    "            positional_index[term][int(docid)][t] = [position]\n",
    "        else:\n",
    "            if(t not in positional_index[term][int(docid)]):\n",
    "                positional_index[term][int(docid)][t] = [position]\n",
    "            else:\n",
    "                positional_index[term][int(docid)][t].append(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xml.dom import minidom\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "docids = []\n",
    "def construct_positional_indexes(docs_path, data_format = 'xml'):\n",
    "    if(data_format == 'xml'):\n",
    "        mydoc = minidom.parse(docs_path)\n",
    "        texts = mydoc.getElementsByTagName('text')\n",
    "        titles = mydoc.getElementsByTagName('title')\n",
    "        ids = mydoc.getElementsByTagName('id')\n",
    "        tree = ET.parse(docs_path)\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            for c in child:\n",
    "                if(c.tag == '{http://www.mediawiki.org/xml/export-0.10/}id'):\n",
    "                    docids.append(int(c.text))\n",
    "        for i in range(len(texts)):\n",
    "            A = prepare_text(titles[i].firstChild.data)\n",
    "            B = prepare_text(texts[i].firstChild.data)\n",
    "            for j in range(len(A)):\n",
    "                add_positional(A[j], docids[i], j, 'title')\n",
    "            for j in range(len(B)):\n",
    "                add_positional(B[j], docids[i], j, 'text')\n",
    "    \n",
    "    elif(data_format == 'csv'):\n",
    "        data = pd.read_csv(docs_path)\n",
    "        descs = data.iloc[:, 1].values\n",
    "        titles = data.iloc[:, 14].values\n",
    "        for i in range(len(descs)):\n",
    "            docids.append(i)\n",
    "            A = prepare_text(titles[i], 'en')\n",
    "            B = prepare_text(descs[i], 'en')\n",
    "            for j in range(len(A)):\n",
    "                add_positional(A[j], i, j, 'title')\n",
    "            for j in range(len(B)):\n",
    "                add_positional(B[j], i, j, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_positional_indexes('persian.xml')\n",
    "construct_positional_indexes('ted_talks.csv', 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_index = {}\n",
    "def add_bigram(word):\n",
    "    new_word = \"$\" + word + \"$\"\n",
    "    for i in range(len(new_word) - 1):\n",
    "        bi = new_word[i] + new_word[i + 1]\n",
    "        if(bi not in bigram_index.keys()):\n",
    "            bigram_index[bi] = [word]\n",
    "        else:\n",
    "            if(word not in bigram_index[bi]):\n",
    "                bigram_index[bi].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_bigram_indexes(docs_path, data_format = 'xml'):\n",
    "    if(data_format == 'xml'):\n",
    "        mydoc = minidom.parse(docs_path)\n",
    "        texts = mydoc.getElementsByTagName('text')\n",
    "        titles = mydoc.getElementsByTagName('title')\n",
    "        for i in range(len(texts)):\n",
    "            A = prepare_text(titles[i].firstChild.data)\n",
    "            B = prepare_text(texts[i].firstChild.data)\n",
    "            for j in range(len(A)):\n",
    "                add_bigram(A[j])\n",
    "            for j in range(len(B)):\n",
    "                add_bigram(B[j])\n",
    "                \n",
    "    elif(data_format == 'csv'):\n",
    "        data = pd.read_csv(docs_path)\n",
    "        descs = data.iloc[:, 1].values\n",
    "        titles = data.iloc[:, 14].values\n",
    "        for i in range(len(descs)):\n",
    "            A = prepare_text(titles[i], 'en')\n",
    "            B = prepare_text(descs[i], 'en')\n",
    "            for j in range(len(A)):\n",
    "                add_bigram(A[j])\n",
    "            for j in range(len(B)):\n",
    "                add_bigram(B[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "construct_bigram_indexes('persian.xml')\n",
    "construct_bigram_indexes('ted_talks.csv', 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document_to_indexes(docs_path, doc_num, file_format = 'xml'):\n",
    "    if(file_format == 'xml'):\n",
    "        mydoc = minidom.parse(docs_path)\n",
    "        texts = mydoc.getElementsByTagName('text')\n",
    "        titles = mydoc.getElementsByTagName('title')\n",
    "        if(doc_num not in docids):\n",
    "            docs = []\n",
    "            tree = ET.parse(docs_path)\n",
    "            root = tree.getroot()\n",
    "            for child in root:\n",
    "                for c in child:\n",
    "                    if(c.tag == '{http://www.mediawiki.org/xml/export-0.10/}id'):\n",
    "                        docs.append(int(c.text))\n",
    "            i = docs.index(doc_num)\n",
    "            A = prepare_text(titles[i].firstChild.data)\n",
    "            B = prepare_text(texts[i].firstChild.data)\n",
    "            for j in range(len(A)):\n",
    "                add_positional(A[j], doc_num, j, 'title')\n",
    "\n",
    "            for j in range(len(B)):\n",
    "                add_positional(B[j], doc_num, j, 'text')\n",
    "            docids.append(doc_num)\n",
    "            \n",
    "    elif(file_format == 'csv'):\n",
    "        data = pd.read_csv(docs_path)\n",
    "        descs = data.iloc[:, 1].values\n",
    "        titles = data.iloc[:, 14].values\n",
    "        if(doc_num not in docids):\n",
    "            docs = []\n",
    "            for i in range(len(descs)):\n",
    "                docs.append(i)\n",
    "            i = docs.index(doc_num)\n",
    "            A = prepare_text(titles[i])\n",
    "            B = prepare_text(descs[i])\n",
    "            for j in range(len(A)):\n",
    "                add_positional(A[j], doc_num, j, 'title')\n",
    "\n",
    "            for j in range(len(B)):\n",
    "                add_positional(B[j], doc_num, j, 'text')\n",
    "            docids.append(doc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_document_to_indexes('persian.xml', 3022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_document_from_indexes(docs_path, doc_num, file_format = 'xml'):\n",
    "    if(file_format == 'xml'):\n",
    "        mydoc = minidom.parse(docs_path)\n",
    "        texts = mydoc.getElementsByTagName('text')\n",
    "        titles = mydoc.getElementsByTagName('title')\n",
    "        if(doc_num in docids):\n",
    "            docs = []\n",
    "            tree = ET.parse(docs_path)\n",
    "            root = tree.getroot()\n",
    "            for child in root:\n",
    "                for c in child:\n",
    "                    if(c.tag == '{http://www.mediawiki.org/xml/export-0.10/}id'):\n",
    "                        docs.append(int(c.text))\n",
    "            i = docs.index(doc_num)\n",
    "            A = prepare_text(titles[i].firstChild.data)\n",
    "            B = prepare_text(texts[i].firstChild.data)\n",
    "            for j in range(len(A)):\n",
    "                if(A[j] in positional_index.keys()):\n",
    "                    if(doc_num in positional_index[A[j]]):\n",
    "                        del positional_index[A[j]][doc_num]\n",
    "                    if(len(positional_index[A[j]].keys()) == 0):\n",
    "                        del positional_index[A[j]]\n",
    "\n",
    "            for j in range(len(B)):\n",
    "                if(B[j] in positional_index.keys()):\n",
    "                    if(doc_num in positional_index[B[j]]):\n",
    "                        del positional_index[B[j]][doc_num]\n",
    "                    if(len(positional_index[B[j]].keys()) == 0):\n",
    "                        del positional_index[B[j]]\n",
    "            docids.remove(doc_num)\n",
    "            \n",
    "    elif(file_format == 'csv'):\n",
    "        data = pd.read_csv(docs_path)\n",
    "        descs = data.iloc[:, 1].values\n",
    "        titles = data.iloc[:, 14].values\n",
    "        if(doc_num in docids):\n",
    "            docs = []\n",
    "            for i in range(len(descs)):\n",
    "                docs.append(i)\n",
    "            i = docs.index(doc_num)\n",
    "            A = prepare_text(titles[i])\n",
    "            B = prepare_text(descs[i])\n",
    "            for j in range(len(A)):\n",
    "                if(A[j] in positional_index.keys()):\n",
    "                    if(doc_num in positional_index[A[j]]):\n",
    "                        del positional_index[A[j]][doc_num]\n",
    "                    if(len(positional_index[A[j]].keys()) == 0):\n",
    "                        del positional_index[A[j]]\n",
    "\n",
    "            for j in range(len(B)):\n",
    "                if(B[j] in positional_index.keys()):\n",
    "                    if(doc_num in positional_index[B[j]]):\n",
    "                        del positional_index[B[j]][doc_num]\n",
    "                    if(len(positional_index[B[j]].keys()) == 0):\n",
    "                        del positional_index[B[j]]\n",
    "            docids.remove(doc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delete_document_from_indexes('persian.xml', 3022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_positional_index(destination):\n",
    "    j = json.dumps(positional_index)\n",
    "    f = open(destination,\"w\")\n",
    "    f.write(j)\n",
    "    f.close()\n",
    "    \n",
    "def save_bigram_index(destination):\n",
    "    j = json.dumps(bigram_index)\n",
    "    f = open(destination,\"w\")\n",
    "    f.write(j)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_positional_index('positional.json')\n",
    "save_bigram_index('bigram.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_positional_index(source):\n",
    "    with open(source) as json_file:\n",
    "        positional_index = json.load(json_file)\n",
    "\n",
    "def load_bigram_index(source):\n",
    "    with open(source) as json_file:\n",
    "        bigram_index = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_positional_index('positional.json')\n",
    "load_bigram_index('bigram.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 3 : Index compression</h1>\n",
    "<div>In this section we are going to use variable byte coding and gamma coding to compress our positional index postiong lists and compare the index size in all 3 methods. We also need to write a decode function for both variable byte coding and gamma coding technique.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "for term in positional_index:\n",
    "    inverted_index[term] = sorted(list(positional_index[term].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3004240\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "from objsize import get_deep_size\n",
    "\n",
    "print(get_deep_size(inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def log2(x):\n",
    "    return log(x, 2)\n",
    "\n",
    "def unary(N):\n",
    "    return N*\"1\" + \"0\"\n",
    "\n",
    "def binary(x, l = 1): \n",
    "    s = '{0:0%db}' % l \n",
    "    return s.format(x)\n",
    "\n",
    "def gamma(N):\n",
    "    if(N == 0):\n",
    "        return '0'\n",
    "    n = int(log2(N)) \n",
    "    b = N - 2**(int(log2(N))) \n",
    "    l = int(log2(N)) \n",
    "    return unary(n) + binary(b, l) \n",
    "    \n",
    "def gamma_coding(index):\n",
    "    result = {}\n",
    "    for term in index:\n",
    "        posting_list = index[term]\n",
    "        temp = gamma(posting_list[0])\n",
    "        for i in range(1, len(posting_list)):\n",
    "            temp += gamma(posting_list[i] - posting_list[i - 1])\n",
    "        result[term] = temp\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2378618\n"
     ]
    }
   ],
   "source": [
    "gamma_coding_inverted_index = gamma_coding(inverted_index)\n",
    "print(get_deep_size(gamma_coding_inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_byte(N):\n",
    "    b = binary(N)\n",
    "    temp = \"\"\n",
    "    k = 0\n",
    "    for i in range(1, len(b) + 1):\n",
    "        if(k == 7):\n",
    "            temp = '0' + temp\n",
    "            k = 0\n",
    "        temp = b[-i] + temp\n",
    "        k += 1\n",
    "    for j in range(8 - k):\n",
    "        temp = '0' + temp\n",
    "    temp = temp[:-8] + '1' + temp[-7:]\n",
    "    return temp\n",
    "\n",
    "def variable_byte_code(index):\n",
    "    result = {}\n",
    "    for term in index:\n",
    "        posting_list = index[term]\n",
    "        temp = variable_byte(posting_list[0])\n",
    "        for i in range(1, len(posting_list)):\n",
    "            temp += variable_byte(posting_list[i] - posting_list[i - 1])\n",
    "        result[term] = temp\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2447567\n"
     ]
    }
   ],
   "source": [
    "variable_byte_coding_inverted_index = variable_byte_code(inverted_index)\n",
    "print(get_deep_size(variable_byte_coding_inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_gamma(code):\n",
    "    \n",
    "    \n",
    "def decode_gamma_coding(encoded):\n",
    "    result = {}\n",
    "    for term in encoded:\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 4 : Query correction</h1>\n",
    "<div>In this section we use the bigram indexing to correct spelling errors in the query. We first use the jaccard distance to find the most likely cases of spelling correction and from that set we use the edit distant measure to find the best answer.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jakard_distance(word1, word2):\n",
    "    w1 = \"$\" + word1 + \"$\"\n",
    "    w2 = \"$\" + word2 + \"$\"\n",
    "    bi1 = []\n",
    "    bi2 = []\n",
    "    for i in range(len(w1) - 1):\n",
    "        temp = w1[i] + w1[i + 1]\n",
    "        if(temp not in bi1):\n",
    "            bi1.append(temp)\n",
    "            \n",
    "    for i in range(len(w2) - 1):\n",
    "        temp = w2[i] + w2[i + 1]\n",
    "        if(temp not in bi2):\n",
    "            bi2.append(temp)\n",
    "    \n",
    "    U = []\n",
    "    M = []\n",
    "    for bi in (bi1 + bi2):\n",
    "        if(bi not in U):\n",
    "            U.append(bi)\n",
    "    \n",
    "    for bi in bi1:\n",
    "        if(bi in bi2):\n",
    "            M.append(bi)\n",
    "    jakard = len(M) / len(U)\n",
    "    return jakard\n",
    "\n",
    "def edit_distance(word1, word2):\n",
    "    minimum = min(len(word1), len(word2))\n",
    "    maximum = max(len(word1), len(word2))\n",
    "    distance = 0\n",
    "    for i in range(minimum):\n",
    "        if(word1[i] != word2[i]):\n",
    "            distance += 1\n",
    "    distance += (maximum - minimum)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_query(query):\n",
    "    words = query.split(\" \")\n",
    "    correct_query = \"\"\n",
    "    for word in words:\n",
    "        if word in positional_index.keys():\n",
    "            correct_query += word\n",
    "            correct_query += \" \"\n",
    "        else:\n",
    "            all1 = []\n",
    "            temp = \"$\" + word + \"$\"\n",
    "            for i in range(len(temp) - 1):\n",
    "                bi = temp[i] + temp[i + 1]\n",
    "                if(bi in bigram_index):\n",
    "                    for bigram in bigram_index[bi]:\n",
    "                        if(bigram not in all1):\n",
    "                            all1.append(bigram)\n",
    "            jakard = []\n",
    "            for a in all1:\n",
    "                jakard.append(jakard_distance(a, word))\n",
    "            J = sorted(jakard, reverse = True)\n",
    "            maximum = J[9]\n",
    "            candidates = []\n",
    "            for i in range(len(jakard)):\n",
    "                if(jakard[i] >= maximum):\n",
    "                    candidates.append(all1[i])\n",
    "            distance = []\n",
    "            for c in candidates:\n",
    "                distance.append(edit_distance(c, word))\n",
    "            minimum = min(distance)\n",
    "            for i in range(len(distance)):\n",
    "                if(distance[i] == minimum):\n",
    "                    result = candidates[i]\n",
    "                    break\n",
    "            correct_query += result\n",
    "            correct_query += \" \"\n",
    "                \n",
    "    return correct_query[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بازی اطلاعات\n",
      "استان شیروان\n",
      "modern institution retreat\n"
     ]
    }
   ],
   "source": [
    "# print(correct_query('باظیابی اتلاعاط'))\n",
    "# print(correct_query('اشتان شیشتان'))\n",
    "# print(correct_query('moderm infornation retreval'))\n",
    "print(positional_index[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
