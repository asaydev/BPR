{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 1 : Data Pre-processing</h1>\n",
    "<div>In this section we build a function that accepts raw text extracted from a doc or a query and after applying lemmitizitation and stemming return an array of tokens (token stream)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PersianStemmer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-15c17554aa89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhazm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPersianStemmer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPersianStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PersianStemmer'"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from PersianStemmer import PersianStemmer\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def prepare_text(raw_text, lang = 'fa'):\n",
    "    if(lang == 'en'):\n",
    "        tokens = word_tokenize(raw_text)\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        porter = PorterStemmer()\n",
    "        prepared_text = []\n",
    "        for word in tokens:\n",
    "            if(tokens.count(word) < 1/15 or len(tokens) < 120):\n",
    "                prepared_text.append(porter.stem(word))\n",
    "        return prepared_text\n",
    "    \n",
    "    elif(lang == 'fa'):\n",
    "        normalizer = Normalizer()\n",
    "        normalized_text = normalizer.normalize(raw_text)\n",
    "        tokenizer = WordTokenizer()\n",
    "        tokenized_text = tokenizer.tokenize(normalized_text)\n",
    "        ps = PersianStemmer()\n",
    "        prepared_text = []\n",
    "        for word in tokenized_text:\n",
    "            if(word[0] >= \"آ\" and word[0] <= \"ی\" and (tokenized_text.count(word) < 1/15 or len(tokenized_text) < 120)):\n",
    "                prepared_text.append(ps.run(word))\n",
    "        return prepared_text\n",
    "\n",
    "raw_text = input()\n",
    "print(prepare_text(raw_text, 'en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 2 : Indexing</h1>\n",
    "<div>In this section we build a function for positional indexing and biword indexing. We then save this indexes and also we have functions for adding and deleting docs in a dynamic way meaning that you don't need to repeat the indexing process from the beginning.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positional_index = {}\n",
    "def add_positional(term, docid, position, t):\n",
    "    if(term not in positional_index.keys()):\n",
    "        positional_index[term] = {}\n",
    "        positional_index[term][docid] = {}\n",
    "        positional_index[term][docid][t] = [position]\n",
    "    else:\n",
    "        if(docid not in positional_index[term].keys()):\n",
    "            positional_index[term][docid] = {}\n",
    "            positional_index[term][docid][t] = [position]\n",
    "        else:\n",
    "            if(t not in positional_index[term][docid]):\n",
    "                positional_index[term][docid][t] = [position]\n",
    "            else:\n",
    "                positional_index[term][docid][t].append(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docids = []\n",
    "def construct_positional_indexes(docs_path):\n",
    "    # TODO \n",
    "    from xml.dom import minidom\n",
    "    mydoc = minidom.parse(docs_path)\n",
    "    texts = mydoc.getElementsByTagName('text')\n",
    "    titles = mydoc.getElementsByTagName('title')\n",
    "    ids = mydoc.getElementsByTagName('id')\n",
    "    import xml.etree.ElementTree as ET\n",
    "    tree = ET.parse(docs_path)\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        for c in child:\n",
    "            if(c.tag == '{http://www.mediawiki.org/xml/export-0.10/}id'):\n",
    "                docids.append(c.text)\n",
    "    for i in range(len(texts)):\n",
    "        A = prepare_text(titles[i].firstChild.data)\n",
    "        B = prepare_text(texts[i].firstChild.data)\n",
    "        for j in range(len(A)):\n",
    "            add_positional(A[j], docids[i], j, 'title')\n",
    "            \n",
    "        for j in range(len(B)):\n",
    "            add_positional(B[j], docids[i], j, 'text')\n",
    "\n",
    "\n",
    "construct_positional_indexes('persian.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_index = {}\n",
    "def add_bigram(word):\n",
    "    new_word = \"$\" + word + \"$\"\n",
    "    for i in range(len(new_word) - 1):\n",
    "        bi = new_word[i] + new_word[i + 1]\n",
    "        if(bi not in bigram_index.keys()):\n",
    "            bigram_index[bi] = [word]\n",
    "        else:\n",
    "            if(word not in bigram_index[bi]):\n",
    "                bigram_index[bi].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_bigram_indexes(docs_path):\n",
    "    # TODO \n",
    "    from xml.dom import minidom\n",
    "    mydoc = minidom.parse(docs_path)\n",
    "    texts = mydoc.getElementsByTagName('text')\n",
    "    titles = mydoc.getElementsByTagName('title')\n",
    "    for i in range(len(texts)):\n",
    "        A = prepare_text(titles[i].firstChild.data)\n",
    "        B = prepare_text(texts[i].firstChild.data)\n",
    "        for j in range(len(A)):\n",
    "            add_bigram(A[j])\n",
    "            \n",
    "        for j in range(len(B)):\n",
    "            add_bigram(B[j])\n",
    "\n",
    "construct_bigram_indexes('persian.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_document_to_indexes(docs_path, doc_num):\n",
    "    # TODO \n",
    "    from xml.dom import minidom\n",
    "    mydoc = minidom.parse(docs_path)\n",
    "    texts = mydoc.getElementsByTagName('text')\n",
    "    titles = mydoc.getElementsByTagName('title')\n",
    "    if(doc_num not in docids):\n",
    "        docs = []\n",
    "        import xml.etree.ElementTree as ET\n",
    "        tree = ET.parse(docs_path)\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            for c in child:\n",
    "                if(c.tag == '{http://www.mediawiki.org/xml/export-0.10/}id'):\n",
    "                    docs.append(c.text)\n",
    "        i = docs.index(doc_num)\n",
    "        A = prepare_text(titles[i].firstChild.data)\n",
    "        B = prepare_text(texts[i].firstChild.data)\n",
    "        for j in range(len(A)):\n",
    "            add_positional(A[j], doc_num, j, 'title')\n",
    "        \n",
    "        for j in range(len(B)):\n",
    "            add_positional(B[j], doc_num, j, 'text')\n",
    "        \n",
    "        docids.append(doc_num)\n",
    "\n",
    "add_document_to_indexes('persian.xml', '3022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_document_from_indexes(docs_path, doc_num):\n",
    "    from xml.dom import minidom\n",
    "    mydoc = minidom.parse(docs_path)\n",
    "    texts = mydoc.getElementsByTagName('text')\n",
    "    titles = mydoc.getElementsByTagName('title')\n",
    "    if(doc_num in docids):\n",
    "        docs = []\n",
    "        import xml.etree.ElementTree as ET\n",
    "        tree = ET.parse(docs_path)\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            for c in child:\n",
    "                if(c.tag == '{http://www.mediawiki.org/xml/export-0.10/}id'):\n",
    "                    docs.append(c.text)\n",
    "        i = docs.index(doc_num)\n",
    "        A = prepare_text(titles[i].firstChild.data)\n",
    "        B = prepare_text(texts[i].firstChild.data)\n",
    "        for j in range(len(A)):\n",
    "            if(A[j] in positional_index.keys()):\n",
    "                if(doc_num in  positional_index[A[j]]):\n",
    "                    del positional_index[A[j]][doc_num]\n",
    "                if(len(positional_index[A[j]].keys()) == 0):\n",
    "                    del positional_index[A[j]]\n",
    "        \n",
    "        for j in range(len(B)):\n",
    "            if(B[j] in positional_index.keys()):\n",
    "                if(doc_num in  positional_index[B[j]]):\n",
    "                    del positional_index[B[j]][doc_num]\n",
    "                if(len(positional_index[B[j]].keys()) == 0):\n",
    "                    del positional_index[B[j]]\n",
    "        \n",
    "        docids.remove(doc_num)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "delete_document_from_indexes('persian.xml', '3022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_index(destination):\n",
    "    j = json.dumps(positional_index)\n",
    "    f = open(destination,\"w\")\n",
    "    f.write(j)\n",
    "    f.close()\n",
    "    pass\n",
    "\n",
    "save_index('positional.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_index(source):\n",
    "    with open(source) as json_file:\n",
    "        positional_index = json.load(json_file)\n",
    "\n",
    "load_index('positional.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
