{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 1 : Data Pre-processing</h1>\n",
    "<div>In this section we build a function that accepts raw text extracted from a doc or a query and after applying normalization, tokenization, stopword removal and stemming returns an array of tokens (token stream).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xml.dom import minidom\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "persian_words_count = {}\n",
    "mydoc = minidom.parse('persian.xml')\n",
    "texts = mydoc.getElementsByTagName('text')\n",
    "titles = mydoc.getElementsByTagName('title')\n",
    "for i in range(len(texts)):\n",
    "    A = titles[i].firstChild.data.split(' ')\n",
    "    B = texts[i].firstChild.data.split(' ')\n",
    "    for term in A:\n",
    "        if(term in persian_words_count):\n",
    "            persian_words_count[term] += 1\n",
    "        else:\n",
    "            persian_words_count[term] = 1\n",
    "    for term in B:\n",
    "        if(term in persian_words_count):\n",
    "            persian_words_count[term] += 1\n",
    "        else:\n",
    "            persian_words_count[term] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "persian_stopwords_count = []\n",
    "for term in persian_words_count:\n",
    "    persian_stopwords_count.append((term, persian_words_count[term]))\n",
    "persian_stopwords_count.sort(key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'و', 'در', '=', 'به', 'از', 'که', 'این', 'را', 'با', '|']\n"
     ]
    }
   ],
   "source": [
    "persian_stopwords = []\n",
    "for i in range(11):\n",
    "    persian_stopwords.append(persian_stopwords_count[i][0])\n",
    "print(persian_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words_count = {}\n",
    "data = pd.read_csv('ted_talks.csv')\n",
    "descs = data.iloc[:, 1].values\n",
    "titles = data.iloc[:, 14].values\n",
    "for i in range(len(titles)):\n",
    "    A = titles[i].split(' ')\n",
    "    B = descs[i].split(' ')\n",
    "    for term in A:\n",
    "        if(term in english_words_count):\n",
    "            english_words_count[term] += 1\n",
    "        else:\n",
    "            english_words_count[term] = 1\n",
    "    for term in B:\n",
    "        if(term in english_words_count):\n",
    "            english_words_count[term] += 1\n",
    "        else:\n",
    "            english_words_count[term] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords_count = []\n",
    "for term in english_words_count:\n",
    "    english_stopwords_count.append((term, english_words_count[term]))\n",
    "english_stopwords_count.sort(key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'of', 'to', 'a', 'in', '--']\n"
     ]
    }
   ],
   "source": [
    "english_stopwords = []\n",
    "for i in range(7):\n",
    "    english_stopwords.append(english_stopwords_count[i][0])\n",
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-266-9430cac7457c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprepared_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from PersianStemmer import PersianStemmer\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def prepare_text(raw_text, lang = 'fa', stem = True):\n",
    "    if(lang == 'en'):\n",
    "        tokens = word_tokenize(raw_text)\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        porter = PorterStemmer()\n",
    "        prepared_text = []\n",
    "        for word in tokens:\n",
    "            if(word not in english_stopwords):\n",
    "                if(stem):\n",
    "                    prepared_text.append(porter.stem(word))\n",
    "                else:\n",
    "                    prepared_text.append(word)\n",
    "        return prepared_text\n",
    "    \n",
    "    elif(lang == 'fa'):\n",
    "        normalizer = Normalizer()\n",
    "        normalized_text = normalizer.normalize(raw_text)\n",
    "        tokenizer = WordTokenizer()\n",
    "        tokenized_text = tokenizer.tokenize(normalized_text)\n",
    "        ps = PersianStemmer()\n",
    "        prepared_text = []\n",
    "        for word in tokenized_text:\n",
    "            if(word[0] >= \"آ\" and word[0] <= \"ی\" and word not in persian_stopwords):\n",
    "                if(stem):\n",
    "                    prepared_text.append(ps.run(word))\n",
    "                else:\n",
    "                    prepared_text.append(word)\n",
    "        return prepared_text\n",
    "\n",
    "raw_text = input()\n",
    "print(prepare_text(raw_text, 'en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 2 : Indexing</h1>\n",
    "<div>In this section we build a function for positional indexing and biword indexing. We then save this indexes and also we have functions for adding and deleting docs in a dynamic way meaning that you don't need to repeat the indexing process from the beginning.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positional_index = {}\n",
    "def add_positional(term, docid, position, t):\n",
    "    if(term not in positional_index.keys()):\n",
    "        positional_index[term] = {}\n",
    "        positional_index[term][int(docid)] = {}\n",
    "        positional_index[term][int(docid)][t] = [position]\n",
    "    else:\n",
    "        if(int(docid) not in positional_index[term].keys()):\n",
    "            positional_index[term][int(docid)] = {}\n",
    "            positional_index[term][int(docid)][t] = [position]\n",
    "        else:\n",
    "            if(t not in positional_index[term][int(docid)]):\n",
    "                positional_index[term][int(docid)][t] = [position]\n",
    "            else:\n",
    "                positional_index[term][int(docid)][t].append(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "docids = []\n",
    "def construct_positional_indexes(docs_path, data_format = 'xml'):\n",
    "    if(data_format == 'xml'):\n",
    "        mydoc = minidom.parse(docs_path)\n",
    "        texts = mydoc.getElementsByTagName('text')\n",
    "        titles = mydoc.getElementsByTagName('title')\n",
    "        ids = mydoc.getElementsByTagName('id')\n",
    "        tree = ET.parse(docs_path)\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            for c in child:\n",
    "                if(c.tag == '{http://www.mediawiki.org/xml/export-0.10/}id'):\n",
    "                    docids.append(int(c.text))\n",
    "        for i in range(len(texts)):\n",
    "            A = prepare_text(titles[i].firstChild.data)\n",
    "            B = prepare_text(texts[i].firstChild.data)\n",
    "            for j in range(len(A)):\n",
    "                add_positional(A[j], docids[i], j, 'title')\n",
    "            for j in range(len(B)):\n",
    "                add_positional(B[j], docids[i], j, 'text')\n",
    "    \n",
    "    elif(data_format == 'csv'):\n",
    "        data = pd.read_csv(docs_path)\n",
    "        descs = data.iloc[:, 1].values\n",
    "        titles = data.iloc[:, 14].values\n",
    "        for i in range(len(descs)):\n",
    "            docids.append(i)\n",
    "            A = prepare_text(titles[i], 'en')\n",
    "            B = prepare_text(descs[i], 'en')\n",
    "            for j in range(len(A)):\n",
    "                add_positional(A[j], i, j, 'title')\n",
    "            for j in range(len(B)):\n",
    "                add_positional(B[j], i, j, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_positional_indexes('persian.xml')\n",
    "construct_positional_indexes('ted_talks.csv', 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_posting_list(term):\n",
    "    return sorted(list(positional_index[term].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بازی\n",
      "[3014, 4388, 5403, 5786, 6647]\n"
     ]
    }
   ],
   "source": [
    "term = input()\n",
    "print(get_posting_list(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_position(term):\n",
    "    return positional_index['term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بازی\n",
      "{166: {'text': [8]}, 869: {'text': [1]}, 884: {'text': [29]}, 1106: {'text': [34]}, 1861: {'text': [1]}, 1899: {'text': [40]}, 2325: {'text': [33]}, 2426: {'title': [7], 'text': [36, 39]}, 2517: {'text': [31]}}\n"
     ]
    }
   ],
   "source": [
    "term = input()\n",
    "print(get_position(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_index = {}\n",
    "def add_bigram(word):\n",
    "    new_word = \"$\" + word + \"$\"\n",
    "    for i in range(len(new_word) - 1):\n",
    "        bi = new_word[i] + new_word[i + 1]\n",
    "        if(bi not in bigram_index.keys()):\n",
    "            bigram_index[bi] = [word]\n",
    "        else:\n",
    "            if(word not in bigram_index[bi]):\n",
    "                bigram_index[bi].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_bigram_indexes(docs_path, data_format = 'xml'):\n",
    "    if(data_format == 'xml'):\n",
    "        mydoc = minidom.parse(docs_path)\n",
    "        texts = mydoc.getElementsByTagName('text')\n",
    "        titles = mydoc.getElementsByTagName('title')\n",
    "        for i in range(len(texts)):\n",
    "            A = prepare_text(titles[i].firstChild.data, 'fa', False)\n",
    "            B = prepare_text(texts[i].firstChild.data, 'fa', False)\n",
    "            for j in range(len(A)):\n",
    "                add_bigram(A[j])\n",
    "            for j in range(len(B)):\n",
    "                add_bigram(B[j])\n",
    "                \n",
    "    elif(data_format == 'csv'):\n",
    "        data = pd.read_csv(docs_path)\n",
    "        descs = data.iloc[:, 1].values\n",
    "        titles = data.iloc[:, 14].values\n",
    "        for i in range(len(descs)):\n",
    "            A = prepare_text(titles[i], 'en', False)\n",
    "            B = prepare_text(descs[i], 'en', False)\n",
    "            for j in range(len(A)):\n",
    "                add_bigram(A[j])\n",
    "            for j in range(len(B)):\n",
    "                add_bigram(B[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "construct_bigram_indexes('persian.xml')\n",
    "construct_bigram_indexes('ted_talks.csv', 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_bigram(bigram):\n",
    "    return bigram_index[bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "با\n",
      "['بازی', 'مهاباد', 'زبان', 'بار', 'داشته_باشد', 'بادغیس', 'بامیان', 'باستان', 'آلبانی', 'الباسان', 'بانتو', 'با', 'باورها', 'رده|باور', 'انبار-رده|Belief', 'زبان\\u200cشناسی', 'بیابانگرد', 'بیابان', 'جبالیه', 'زبان\\u200cهای', 'بمباران', 'باجلانی', 'باتنه', 'باغ', 'باده', 'صهبا', 'انبار', 'باقی', 'جلال\\u200cآباد', 'سبالو', 'بابازار', 'ضرباهنگ\\u200cهای', 'بالیکسیر', 'بارتین', 'بایبورد', 'بابل', 'باشگاه', 'بازی\\u200cها', 'باستانی', 'راوبال', 'ویکی\\u200cانبار-رده|Geography', 'کشور|آلبانی', 'انبار-رده|Albania', 'فرانسه\\u200cزبانی', 'بالکان', 'آلبانیایی\\u200cزبان', 'انبار-رده|Country', 'انبار-رده|Maps', 'انبار-رده|Countries', 'انبار-رده|Provinces', 'انبار-رده|Asia', 'انبار-رده|Europe', 'انبار|Category', 'انبار-رده|Caucasus', 'انبار-رده|Middle', 'بادن-وورتمبرگ', 'بایرن', 'انبار-رده|Germany', 'آلمانی\\u200cزبان', 'ژرمنی\\u200cزبان', 'انبار-رده|Members', 'ویکی\\u200cانبار-رده|Rivers', 'انبار-رده|Geography', 'انبار-رده', 'دربارهٔ', 'باغ\\u200cهای', 'الفبا', 'درباره', 'کردی\\u200cزبان', 'ارمنی\\u200cزبان', 'فرانسوی\\u200cزبان', 'انگلیسی\\u200cزبان', 'کتاب\\u200cها|زبان', 'آذربایجان', 'مه\\u200cبانگ', 'مهبانگ', 'انبار-رده|Cities', 'انبار-رده|Ottawa', 'گپ/بایگانی', 'عباسیان', 'باب\\u200cالصغیر', 'صالح\\u200cآباد', 'انبار-رده|Graph', 'ویکی\\u200cانبار-رده|File', 'انبار-رده|Wikipedia', 'پرتوبالگان', 'بالای', 'بانگ', 'بارگیر', 'باب', 'انبار-رده|', 'می\\u200cباشند', 'با|کوه', 'پشتیبانی', 'باشند', 'بازرگانی', 'باز', 'اسباط', 'بازنشستگی', 'بابکان', 'بربار', 'اسدآبادی', 'گاوزبان', 'بازی\\u200cهای', 'قرن\\u200cآباد', 'دنبال', 'بازکی', 'زبان|فارسی', 'قهوه\\u200cخانه/بایگانی', 'باغبانی', 'انبار-رده|Tehran', 'انبار-رده|Afghanistan', 'پشتوزبان', 'انبار-رده|South', 'انبار-رده|Stubs', 'فیروزآباد', 'انبار-رده|Bulgaria', 'اسلاوی\\u200cزبان', 'استانبولی\\u200cزبان', 'ویکی\\u200cانبار-رده|Provinces', 'انبار-رده|Balkans', 'انبار-رده|Regions', 'ویکی\\u200cانبار-رده|Streams', 'بازی|ورق\\u200cهای', 'بالاترین', 'می\\u200cباشد', 'انبار-رده|Austria', 'انبار-رده|Tajikistan', 'انبار-رده|National', 'بایدربک', 'انبار-رده|Volcanology', 'انبار-رده|Geology', 'غبار', 'انبار-رده|Poetry', 'انبار-رده|Literature', 'ارتباط\\u200cهای', 'مباحث', 'انبار-رده|Documents', 'انبار-رده|Arab', 'انبار-رده|International', 'کلبا', 'کلباء', 'آذرباد', 'سامبا', 'فوتبال', 'بیباپ', 'رومبا', 'کوبا', 'عبارت\\u200cهای', 'خرم\\u200cآباد', 'بارفروش', 'سبزقباسانان', 'دربار', 'آلاباما', 'الفبای', 'باورند', 'بازرگان', 'بارسلونا', 'عبارت\\u200cاند', 'آبابا', 'بانگی', 'باکو', 'بیلبائو', 'عشق\\u200cآباد', 'مین\\u200cباشیان', 'دورباش\\u200cدار', 'فارسی\\u200cزبان', 'باختری', 'باشد', 'آذربایجانی', 'خیابان', 'هرابال', 'بازیکن']\n"
     ]
    }
   ],
   "source": [
    "bigram = input()\n",
    "print(find_bigram(bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document_to_indexes(docs_path, doc_num, file_format = 'xml'):\n",
    "    if(file_format == 'xml'):\n",
    "        mydoc = minidom.parse(docs_path)\n",
    "        texts = mydoc.getElementsByTagName('text')\n",
    "        titles = mydoc.getElementsByTagName('title')\n",
    "        if(doc_num not in docids):\n",
    "            docs = []\n",
    "            tree = ET.parse(docs_path)\n",
    "            root = tree.getroot()\n",
    "            for child in root:\n",
    "                for c in child:\n",
    "                    if(c.tag == '{http://www.mediawiki.org/xml/export-0.10/}id'):\n",
    "                        docs.append(int(c.text))\n",
    "            i = docs.index(doc_num)\n",
    "            A = prepare_text(titles[i].firstChild.data)\n",
    "            B = prepare_text(texts[i].firstChild.data)\n",
    "            for j in range(len(A)):\n",
    "                add_positional(A[j], doc_num, j, 'title')\n",
    "\n",
    "            for j in range(len(B)):\n",
    "                add_positional(B[j], doc_num, j, 'text')\n",
    "            docids.append(doc_num)\n",
    "            \n",
    "    elif(file_format == 'csv'):\n",
    "        data = pd.read_csv(docs_path)\n",
    "        descs = data.iloc[:, 1].values\n",
    "        titles = data.iloc[:, 14].values\n",
    "        if(doc_num not in docids):\n",
    "            docs = []\n",
    "            for i in range(len(descs)):\n",
    "                docs.append(i)\n",
    "            i = docs.index(doc_num)\n",
    "            A = prepare_text(titles[i])\n",
    "            B = prepare_text(descs[i])\n",
    "            for j in range(len(A)):\n",
    "                add_positional(A[j], doc_num, j, 'title')\n",
    "\n",
    "            for j in range(len(B)):\n",
    "                add_positional(B[j], doc_num, j, 'text')\n",
    "            docids.append(doc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_document_to_indexes('persian.xml', 3022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_document_from_indexes(docs_path, doc_num, file_format = 'xml'):\n",
    "    if(file_format == 'xml'):\n",
    "        mydoc = minidom.parse(docs_path)\n",
    "        texts = mydoc.getElementsByTagName('text')\n",
    "        titles = mydoc.getElementsByTagName('title')\n",
    "        if(doc_num in docids):\n",
    "            docs = []\n",
    "            tree = ET.parse(docs_path)\n",
    "            root = tree.getroot()\n",
    "            for child in root:\n",
    "                for c in child:\n",
    "                    if(c.tag == '{http://www.mediawiki.org/xml/export-0.10/}id'):\n",
    "                        docs.append(int(c.text))\n",
    "            i = docs.index(doc_num)\n",
    "            A = prepare_text(titles[i].firstChild.data)\n",
    "            B = prepare_text(texts[i].firstChild.data)\n",
    "            for j in range(len(A)):\n",
    "                if(A[j] in positional_index.keys()):\n",
    "                    if(doc_num in positional_index[A[j]]):\n",
    "                        del positional_index[A[j]][doc_num]\n",
    "                    if(len(positional_index[A[j]].keys()) == 0):\n",
    "                        del positional_index[A[j]]\n",
    "\n",
    "            for j in range(len(B)):\n",
    "                if(B[j] in positional_index.keys()):\n",
    "                    if(doc_num in positional_index[B[j]]):\n",
    "                        del positional_index[B[j]][doc_num]\n",
    "                    if(len(positional_index[B[j]].keys()) == 0):\n",
    "                        del positional_index[B[j]]\n",
    "            docids.remove(doc_num)\n",
    "            \n",
    "    elif(file_format == 'csv'):\n",
    "        data = pd.read_csv(docs_path)\n",
    "        descs = data.iloc[:, 1].values\n",
    "        titles = data.iloc[:, 14].values\n",
    "        if(doc_num in docids):\n",
    "            docs = []\n",
    "            for i in range(len(descs)):\n",
    "                docs.append(i)\n",
    "            i = docs.index(doc_num)\n",
    "            A = prepare_text(titles[i])\n",
    "            B = prepare_text(descs[i])\n",
    "            for j in range(len(A)):\n",
    "                if(A[j] in positional_index.keys()):\n",
    "                    if(doc_num in positional_index[A[j]]):\n",
    "                        del positional_index[A[j]][doc_num]\n",
    "                    if(len(positional_index[A[j]].keys()) == 0):\n",
    "                        del positional_index[A[j]]\n",
    "\n",
    "            for j in range(len(B)):\n",
    "                if(B[j] in positional_index.keys()):\n",
    "                    if(doc_num in positional_index[B[j]]):\n",
    "                        del positional_index[B[j]][doc_num]\n",
    "                    if(len(positional_index[B[j]].keys()) == 0):\n",
    "                        del positional_index[B[j]]\n",
    "            docids.remove(doc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delete_document_from_indexes('persian.xml', 3022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "    \n",
    "def save_index(destination, index):\n",
    "    j = json.dumps(index)\n",
    "    f = open(destination,\"w\")\n",
    "    f.write(j)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_positional_index('positional.json', positional_index)\n",
    "save_bigram_index('bigram.json', bigram_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_index(source):\n",
    "    with open(source) as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_positional_index('positional.json')\n",
    "load_bigram_index('bigram.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 3 : Index compression</h1>\n",
    "<div>In this section we are going to use variable byte coding and gamma coding to compress our positional index postiong lists and compare the index size in all 3 methods. We also need to write a decode function for both variable byte coding and gamma coding technique.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45674344\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "from objsize import get_deep_size\n",
    "\n",
    "print(get_deep_size(positional_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def log2(x):\n",
    "    return log(x, 2)\n",
    "\n",
    "def unary(N):\n",
    "    return N*\"1\" + \"0\"\n",
    "\n",
    "def binary(x, l = 1): \n",
    "    s = '{0:0%db}' % l \n",
    "    return s.format(x)\n",
    "\n",
    "def gamma(N):\n",
    "    if(N == 0):\n",
    "        return '2'\n",
    "    if(N == 1):\n",
    "        return '0'\n",
    "    n = int(log2(N)) \n",
    "    b = N - 2**(int(log2(N))) \n",
    "    l = int(log2(N)) \n",
    "    return unary(n) + binary(b, l) \n",
    "    \n",
    "def gamma_coding(index):\n",
    "    result = {}\n",
    "    for term in index:\n",
    "        for doc in index[term]:\n",
    "            for t in index[term][doc]:\n",
    "                posting_list = index[term][doc][t]\n",
    "                temp = gamma(posting_list[0])\n",
    "                for i in range(1, len(posting_list)):\n",
    "                    temp += gamma(posting_list[i] - posting_list[i - 1])\n",
    "                if(term in result):\n",
    "                    if(doc in result[term]):\n",
    "                        result[term][doc][t] = temp\n",
    "                    else:\n",
    "                        result[term][doc] = {}\n",
    "                        result[term][doc][t] = temp\n",
    "                else:\n",
    "                    result[term] = {}\n",
    "                    result[term][doc] = {}\n",
    "                    result[term][doc][t] = temp\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42771238\n"
     ]
    }
   ],
   "source": [
    "gamma_coding_positional_index = gamma_coding(positional_index)\n",
    "print(get_deep_size(gamma_coding_positional_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_index('gamma_encoded.json', gamma_coding_positional_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_byte(N):\n",
    "    b = binary(N)\n",
    "    temp = \"\"\n",
    "    k = 0\n",
    "    for i in range(1, len(b) + 1):\n",
    "        if(k == 7):\n",
    "            temp = '0' + temp\n",
    "            k = 0\n",
    "        temp = b[-i] + temp\n",
    "        k += 1\n",
    "    for j in range(8 - k):\n",
    "        temp = '0' + temp\n",
    "    temp = temp[:-8] + '1' + temp[-7:]\n",
    "    return temp\n",
    "\n",
    "def variable_byte_code(index):\n",
    "    result = {}\n",
    "    for term in index:\n",
    "        for doc in index[term]:\n",
    "            for t in index[term][doc]:\n",
    "                posting_list = index[term][doc][t]\n",
    "                temp = variable_byte(posting_list[0])\n",
    "                for i in range(1, len(posting_list)):\n",
    "                    temp += variable_byte(posting_list[i] - posting_list[i - 1])\n",
    "                if(term in result):\n",
    "                    if(doc in result[term]):\n",
    "                        result[term][doc][t] = temp\n",
    "                    else:\n",
    "                        result[term][doc] = {}\n",
    "                        result[term][doc][t] = temp\n",
    "                else:\n",
    "                    result[term] = {}\n",
    "                    result[term][doc] = {}\n",
    "                    result[term][doc][t] = temp\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3014: {'title': '10000000'}, 4388: {'title': '10000001'}, 5403: {'text': '10011110'}, 5786: {'text': '100100101000010110001000'}, 6647: {'title': '10000000', 'text': '10000000'}}\n",
      "43431906\n"
     ]
    }
   ],
   "source": [
    "variable_byte_coding_positional_index = variable_byte_code(positional_index)\n",
    "print(get_deep_size(variable_byte_coding_positional_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_index('variable_byte_encoded.json', variable_byte_coding_positional_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_gamma(code):\n",
    "    if(code[0] == 0):\n",
    "        return 0\n",
    "    k = 0\n",
    "    for i in range(len(code)):\n",
    "        if(code[i] == '1'):\n",
    "            k += 1\n",
    "        elif(code[i] == '0'):\n",
    "            break\n",
    "    return int('1' + code[k + 1:], 2)\n",
    "    \n",
    "def decode_gamma_coding(encoded):\n",
    "    result = {}\n",
    "    for term in encoded:\n",
    "        for doc in encoded[term]:\n",
    "            for t in encoded[term][doc]:\n",
    "                a = 0\n",
    "                posting_list = []\n",
    "                code = encoded[term][doc][t]\n",
    "                while(True):\n",
    "                    if(len(code) == 0):\n",
    "                        break\n",
    "                    if(code[0] == '0'):\n",
    "                        a += 1\n",
    "                        posting_list.append(a)\n",
    "                        code = code[1:]\n",
    "                    elif(code[0] == '2'):\n",
    "                        posting_list.append(a)\n",
    "                        code = code[1:]\n",
    "                    else:\n",
    "                        k = 0\n",
    "                        for i in range(len(code)):\n",
    "                            if(code[i] == '1'):\n",
    "                                k += 1\n",
    "                            else:\n",
    "                                break\n",
    "                        temp = code[0: 2*k + 1]\n",
    "                        code = code[2*k + 1:]\n",
    "                        a = a + decode_gamma(temp)\n",
    "                        posting_list.append(a)\n",
    "                if(term in result):\n",
    "                    if(doc in result[term]):\n",
    "                        result[term][doc][t] = posting_list\n",
    "                    else:\n",
    "                        result[term][doc] = {}\n",
    "                        result[term][doc][t] = posting_list\n",
    "                else:\n",
    "                    result[term] = {}\n",
    "                    result[term][doc] = {}\n",
    "                    result[term][doc][t] = posting_list\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3014: {'title': [0]}, 4388: {'title': [1]}, 5403: {'text': [30]}, 5786: {'text': [18, 23, 31]}, 6647: {'title': [0], 'text': [0]}}\n"
     ]
    }
   ],
   "source": [
    "decoded_gamma = decode_gamma_coding(gamma_coding_positional_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_index(index1, index2):\n",
    "    if(list(index1.keys()) != list(index2.keys())):\n",
    "        return False\n",
    "    flag = True\n",
    "    for key in index1:\n",
    "        if(index1[key] != index2[key]):\n",
    "            flag = False\n",
    "            break\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(compare_index(positional_index, decoded_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_variable_byte(index):\n",
    "    result = {}\n",
    "    for term in index:\n",
    "        for doc in index[term]:\n",
    "            for t in index[term][doc]:\n",
    "                code = index[term][doc][t]\n",
    "                posting_list = []\n",
    "                a = 0\n",
    "                r = \"\"\n",
    "                while(True):\n",
    "                    if(len(code) == 0):\n",
    "                        break\n",
    "                    temp = code[0:8]\n",
    "                    code = code[8:]\n",
    "                    r += temp[1:8]\n",
    "                    if(temp[0] == '1'):\n",
    "                        a += int(r, 2)\n",
    "                        posting_list.append(a)\n",
    "                        r = \"\"\n",
    "                if(term in result):\n",
    "                    if(doc in result[term]):\n",
    "                        result[term][doc][t] = posting_list\n",
    "                    else:\n",
    "                        result[term][doc] = {}\n",
    "                        result[term][doc][t] = posting_list\n",
    "                else:\n",
    "                    result[term] = {}\n",
    "                    result[term][doc] = {}\n",
    "                    result[term][doc][t] = posting_list\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_variable_byte = decode_variable_byte(variable_byte_coding_positional_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(compare_index(positional_index, decoded_variable_byte))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 4 : Query correction</h1>\n",
    "<div>In this section we use the bigram indexing to correct spelling errors in the query. We first use the jaccard distance to find the most likely cases of spelling correction and from that set we use the edit distant measure to find the best answer.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jakard_distance(word1, word2):\n",
    "    w1 = \"$\" + word1 + \"$\"\n",
    "    w2 = \"$\" + word2 + \"$\"\n",
    "    bi1 = []\n",
    "    bi2 = []\n",
    "    for i in range(len(w1) - 1):\n",
    "        temp = w1[i] + w1[i + 1]\n",
    "        if(temp not in bi1):\n",
    "            bi1.append(temp)\n",
    "            \n",
    "    for i in range(len(w2) - 1):\n",
    "        temp = w2[i] + w2[i + 1]\n",
    "        if(temp not in bi2):\n",
    "            bi2.append(temp)\n",
    "    \n",
    "    U = []\n",
    "    M = []\n",
    "    for bi in (bi1 + bi2):\n",
    "        if(bi not in U):\n",
    "            U.append(bi)\n",
    "    \n",
    "    for bi in bi1:\n",
    "        if(bi in bi2):\n",
    "            M.append(bi)\n",
    "    jakard = len(M) / len(U)\n",
    "    return jakard\n",
    "\n",
    "def edit_distance(word1, word2):\n",
    "    minimum = min(len(word1), len(word2))\n",
    "    maximum = max(len(word1), len(word2))\n",
    "    distance = 0\n",
    "    for i in range(minimum):\n",
    "        if(word1[i] != word2[i]):\n",
    "            distance += 1\n",
    "    distance += (maximum - minimum)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4666666666666667\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(jakard_distance('information', 'informant'))\n",
    "print(edit_distance('modern', 'mortem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_query(query):\n",
    "    words = query.split(\" \")\n",
    "    correct_query = \"\"\n",
    "    for word in words:\n",
    "        if word in positional_index.keys():\n",
    "            correct_query += word\n",
    "            correct_query += \" \"\n",
    "        else:\n",
    "            all1 = []\n",
    "            temp = \"$\" + word + \"$\"\n",
    "            for i in range(len(temp) - 1):\n",
    "                bi = temp[i] + temp[i + 1]\n",
    "                if(bi in bigram_index):\n",
    "                    for bigram in bigram_index[bi]:\n",
    "                        if(bigram not in all1):\n",
    "                            all1.append(bigram)\n",
    "            jakard = []\n",
    "            for a in all1:\n",
    "                jakard.append(jakard_distance(a, word))\n",
    "            J = sorted(jakard, reverse = True)\n",
    "            maximum = J[9]\n",
    "            candidates = []\n",
    "            for i in range(len(jakard)):\n",
    "                if(jakard[i] >= maximum):\n",
    "                    candidates.append(all1[i])\n",
    "            distance = []\n",
    "            for c in candidates:\n",
    "                distance.append(edit_distance(c, word))\n",
    "            minimum = min(distance)\n",
    "            for i in range(len(distance)):\n",
    "                if(distance[i] == minimum):\n",
    "                    result = candidates[i]\n",
    "                    break\n",
    "            correct_query += result\n",
    "            correct_query += \" \"\n",
    "                \n",
    "    return correct_query[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بازی اطلاعات\n",
      "استان شیروان\n",
      "modern information retreat\n",
      "suspicious space ship in afghanistan\n"
     ]
    }
   ],
   "source": [
    "print(correct_query('باظیابی اتلاعاط'))\n",
    "print(correct_query('اشتان شیشتان'))\n",
    "print(correct_query('moderm infornation retreval'))\n",
    "print(correct_query('susspicious spac ship in afkhanistan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 5 : Query search</h1>\n",
    "<div>In this section we implement a weighted search method based on tf-idf and also an approximate search function with a specified window size.<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def search(query, lang = 'fa', weight=2, returned = 10, all_docs = None):\n",
    "    relevant_docs = []\n",
    "    N = len(docids)\n",
    "    t = len(positional_index.keys())\n",
    "\n",
    "    idf = []\n",
    "    words = prepare_text(query, lang)\n",
    "#     essential = prune_text(query)\n",
    "    essential = []\n",
    "            \n",
    "    q = []\n",
    "    all1 = []\n",
    "    for word in positional_index.keys():\n",
    "        all1.append(word)\n",
    "        \n",
    "\n",
    "    query_terms = []\n",
    "    for word in words:\n",
    "        if(word not in query_terms):\n",
    "            query_terms.append(word)\n",
    "\n",
    "    q1 = []\n",
    "    for term in query_terms:\n",
    "        q1.append(1 + math.log(words.count(term), 10))\n",
    "    \n",
    "\n",
    "    for term in query_terms:\n",
    "\n",
    "        temp1 = 0\n",
    "        for d in docids:\n",
    "            if(d in positional_index[term].keys()):\n",
    "                temp1 += 1\n",
    "\n",
    "        idf.append(math.log(N / (temp1), 10))\n",
    "        \n",
    "    score = []\n",
    "    docids1 = docids\n",
    "    if(all_docs != None):\n",
    "        docids1 = all_docs\n",
    "    \n",
    "    for i in docids1:\n",
    "        if(lang == 'en' and i > 3000):\n",
    "            continue\n",
    "        if(lang == 'fa' and i < 3000):\n",
    "            continue\n",
    "        flag = False\n",
    "        f = True\n",
    "            \n",
    "        if(len(essential) != 0):\n",
    "            for word in essential:\n",
    "                if(i not in positional_index[word]):\n",
    "                    flag = True\n",
    "                    break\n",
    "                else:\n",
    "                    if('text' not in positional_index[word][i].keys()):\n",
    "                        flag = True\n",
    "                        break\n",
    "            if(flag == True):\n",
    "                continue\n",
    "            A = positional_index[essential[0]][i]['text'] \n",
    "            f = False\n",
    "            for j in A:\n",
    "                temp = 0\n",
    "                for k in range(len(essential)):\n",
    "                    if((j + k) in positional_index[essential[k]][i]['text']):\n",
    "                        temp += 1\n",
    "                if(temp == len(essential)):\n",
    "                    f = True\n",
    "                    break\n",
    "\n",
    "        if(flag == True):\n",
    "            continue\n",
    "        \n",
    "        if(f == False):\n",
    "            continue\n",
    "                \n",
    "        s = 0\n",
    "        length_title = 0\n",
    "        length_text = 0\n",
    "        for term in query_terms:\n",
    "            if(i not in positional_index[term]):\n",
    "                length_title += 0\n",
    "                length_text += 0\n",
    "            else:\n",
    "                if('title' not in positional_index[term][i].keys()):\n",
    "                    length_title += 0\n",
    "                else:\n",
    "                    length_title += (len(positional_index[term][i]['title'])) ** 2\n",
    "\n",
    "                if('text' not in positional_index[term][i].keys()):\n",
    "                    length_text += 0\n",
    "                else:\n",
    "                    length_text += (len(positional_index[term][i]['text'])) ** 2\n",
    "        length_title = math.sqrt(length_title)\n",
    "        length_text = math.sqrt(length_text)\n",
    "        v = 0\n",
    "        for term in query_terms:\n",
    "            if(i not in positional_index[term]):\n",
    "                s += 0\n",
    "            else:\n",
    "                if('title' not in positional_index[term][i].keys()):\n",
    "                    s += 0\n",
    "                else:\n",
    "                    temp = len(positional_index[term][i]['title'])\n",
    "                    s += weight * ((1 + math.log(temp, 10)) * idf[v] * q1[v]) / length_title\n",
    "\n",
    "                if('text' not in positional_index[term][i].keys()):\n",
    "                    s += 0\n",
    "                else:\n",
    "                    temp = len(positional_index[term][i]['text'])\n",
    "                    s += ((1 + math.log(temp, 10)) * idf[v] * q1[v]) / length_text\n",
    "            v += 1\n",
    "        score.append([i, s])\n",
    "\n",
    "    result = sorted(score, key = lambda x:x[1], reverse = True)\n",
    "    z = min(returned, len(result))\n",
    "    for i in range(z):\n",
    "        relevant_docs.append(result[i][0])\n",
    "        \n",
    "    return relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[277, 278, 1771, 2498, 2455, 1805, 1489, 331, 822, 1398]\n",
      "[6647, 6417, 6418, 6753, 3014, 4388, 5403, 7085, 5786, 3016]\n"
     ]
    }
   ],
   "source": [
    "print(search(\"walking a dog\", 'en'))\n",
    "print(search(\"بازی فوتبال\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proximate_search(query, window, lang = 'fa'):\n",
    "    words = prepare_text(query, lang)\n",
    "    docs = []\n",
    "    relevant_docs = []\n",
    "    for word in words:\n",
    "        docs.append(set(positional_index[word].keys()))\n",
    "    all_docs = set.intersection(*docs)\n",
    "    for doc in all_docs:\n",
    "        posting_lists = []\n",
    "        flag = False\n",
    "        for word in words:\n",
    "            if('text' in positional_index[word][doc]):\n",
    "                posting_lists.append(positional_index[word][doc]['text'])\n",
    "            else:\n",
    "                flag = True\n",
    "                break\n",
    "        if(flag):\n",
    "            continue\n",
    "        for p in posting_lists[0]:\n",
    "            flag2 = False\n",
    "            for i in range(1, len(posting_lists)):\n",
    "                if(len(list(filter(lambda x : x <= (p + window) and x >= (p - window), posting_lists[i]))) == 0):\n",
    "                    flag2 = True\n",
    "                    break\n",
    "            if(flag2):\n",
    "                continue\n",
    "            else:\n",
    "                relevant_docs.append(doc)\n",
    "                break\n",
    "    return search(query, lang, all_docs = relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43]\n",
      "[1697, 2231, 1342, 63]\n"
     ]
    }
   ],
   "source": [
    "print(proximate_search(\"demonic lyrics\", 2, 'en'))\n",
    "print(proximate_search(\"rest world\", 4, 'en'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
